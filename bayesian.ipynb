{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37332bitdc0bd65fb48d4c05abf511b9e7dccc57",
   "display_name": "Python 3.7.3 32-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Loan Default Prediction with Naive Bayes model\n",
    "#### Implemented from scratch\n",
    "###### Author: Wendy Liu\n",
    "<br/>Before you start, click to download the data set (both training set and test set) from the [source website](https://tianchi.aliyun.com/competition/entrance/531830/information?lang=en-us) or download it from [Google Drive](https://drive.google.com/drive/folders/1FHJycjwZIBU-rHuDvBtTves-RSn9orc9?usp=sharing) instead."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from defaultlist import defaultlist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "source": [
    "#### Import dataset\n",
    "#### The default dataset is the smallest sample set with a shape of 150*32.\n",
    "##### *Note: Please insert your file path and file name here*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \".\\\\datasets\\\\\"\n",
    "filename = 'sample-150.csv'\n",
    "# filename = 'sample-100k.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The file size (including the header row) is 152\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORT DATASET while Dropping unusable columns\n",
    "    Dropped column ID: 32+\n",
    "'''\n",
    "\n",
    "dataset = defaultlist()\n",
    "with open(path + filename, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if not row:\n",
    "            continue\n",
    "        dataset.append(row[:32])\n",
    "dataset = np.asarray(dataset)\n",
    "\n",
    "print(\"The file size (including the header row) is\", len(dataset))"
   ]
  },
  {
   "source": [
    "#### Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'loanAmnt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-aac4e8d93ba4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnewDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-aac4e8d93ba4>\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                     \u001b[0mnewLine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoFloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'loanAmnt'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Convert data types\n",
    "    Int type column ID: 0,2,9,11,13,27,28\n",
    "    Float type column ID: 1,3,4,10,14,15,18,19,20,21,26,27,30,31\n",
    "    Str type column ID: 5,6,7,8,12,16,17,22,23,24,29\n",
    "'''\n",
    "\n",
    "\"If you encounter error in this section, please rerun the entire script. The error usually goes away.\"\n",
    "\n",
    "def convert(dataset):\n",
    "    toInt = [0,2,9,11,13,27,28]\n",
    "    toFloat = [1,3,4,10,14,15,18,19,20,21,26,27,30,31]\n",
    "    newDataset = list()\n",
    "    \n",
    "    for i in range(0, len(dataset[0])):\n",
    "        line = list()\n",
    "        line = np.asarray(dataset)[:,i]\n",
    "        # line = np.array(dataset)[i]\n",
    "        # print(i,line[0])\n",
    "        newLine = list()\n",
    "        newLine.append(line[0])\n",
    "        if (i) in toInt:\n",
    "            for j in range(1, len(line)):\n",
    "                if line is not None:\n",
    "                    newLine.append(int(line[j]))\n",
    "        elif (i) in toFloat:\n",
    "            for j in range(1, len(line)):\n",
    "                if line is not None:\n",
    "                    newLine.append(float(line[j]))\n",
    "        else:\n",
    "            for j in range(1, len(line)):\n",
    "                if line is not None:\n",
    "                    newLine.append(str(line[j]))\n",
    "        newDataset.append(newLine)\n",
    "    return newDataset\n",
    "\n",
    "dataset = convert(dataset)"
   ]
  },
  {
   "source": [
    "#### Data set split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split training set / test set\n",
    "    class name: isDefault\n",
    "    class value: {0, 1}\n",
    "'''\n",
    "\n",
    "x_train = list()\n",
    "y_train = list()\n",
    "x_train_str_type_only = list()\n",
    "x_train_num_type_only = list()\n",
    "x_train_gaussian = list()\n",
    "classloc = 13 # isDefault's column id\n",
    "\n",
    "\"Training set\"\n",
    "x_train = np.asarray((dataset[:classloc] + dataset[classloc+1:]), dtype='O')\n",
    "x_train = np.asarray(x_train[:,1:], dtype='O')\n",
    "# print(x_train)\n",
    "\n",
    "y_train = np.asarray(dataset[classloc], dtype='O')\n",
    "y_train = np.asarray(y_train[1:], dtype='O')\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"BEFORE DATASET SPLIT\"\n",
    "\n",
    "labels = [\"Is Default\", \"Not Default\"]\n",
    "y_train_total_size = y_train.size\n",
    "isDefault_count = 0\n",
    "for item in y_train:\n",
    "    if item == 1:\n",
    "        isDefault_count += 1\n",
    "print(isDefault_count, y_train_total_size)\n",
    "sizes = [isDefault_count, y_train_total_size-isDefault_count]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Stratify train/test split with Sklearn\n",
    "'''\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train.T, y_train, stratify=y_train, test_size=0.5)\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Str type column ID: 5,6,7,8,12,29\n",
    "    Note: Column ID after 13 needs to minus one because 'isDefault' (@ index 13) is already taken out from x_train\n",
    "'''\n",
    "x_train_str_type_only = np.asarray((x_train.tolist()[5: 9] + x_train.tolist()[12:13] + x_train.tolist()[28:29]), dtype='str')\n",
    "x_test_str_type_only = np.asarray((x_test.tolist()[5: 9] + x_test.tolist()[12:13] + x_test.tolist()[28:29]), dtype='str')\n",
    "\n",
    "'''\n",
    "Int/Float type columns\n",
    "'''\n",
    "x_train_num_type_only = np.asarray((x_train.tolist()[0:5] + x_train.tolist()[9:12] + x_train.tolist()[13:27] + x_train.tolist()[29:32]), dtype='O')\n",
    "x_test_num_type_only = np.asarray((x_test.tolist()[0:5] + x_test.tolist()[9:12] + x_test.tolist()[13:27] + x_test.tolist()[29:32]), dtype='O')\n",
    "\n",
    "'''\n",
    "Those features that are meaningful for a Gaussian Naive Bayes algorithm\n",
    "'''\n",
    "x_train_gaussian = np.vstack((np.asarray(x_train.tolist()[1:5], dtype='O'), np.asarray(x_train.tolist()[9:11], dtype='O'), np.asarray(x_train.tolist()[19:21], dtype='O')))\n",
    "x_test_gaussian = np.vstack((np.asarray(x_test.tolist()[1:5], dtype='O'), np.asarray(x_test.tolist()[9:11], dtype='O'), np.asarray(x_test.tolist()[19:21], dtype='O')))\n",
    "\n",
    "'''\n",
    "Split data by class\n",
    "'''\n",
    "zero_idx = list()\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 0:\n",
    "        zero_idx.append(i)\n",
    "\n",
    "t = x_train_gaussian.T.tolist()\n",
    "x_train_gaussian0 = list()\n",
    "x_train_gaussian1 = list()\n",
    "idx = 0\n",
    "for i in range(len(t)):\n",
    "    if idx < len(zero_idx) and i == zero_idx[idx]:\n",
    "        x_train_gaussian0.append(t[i])\n",
    "        idx += 1\n",
    "    else:\n",
    "        x_train_gaussian1.append(t[i])\n",
    "x_train_gaussian0 = np.asarray(x_train_gaussian0, dtype='O')\n",
    "x_train_gaussian1 = np.asarray(x_train_gaussian1, dtype='O')\n",
    "\n",
    "t = x_test_gaussian.T.tolist()\n",
    "x_test_gaussian0 = list()\n",
    "x_test_gaussian1 = list()\n",
    "idx = 0\n",
    "for i in range(len(t)):\n",
    "    if idx < len(zero_idx) and i == zero_idx[idx]:\n",
    "        x_test_gaussian0.append(t[i])\n",
    "        idx += 1\n",
    "    else:\n",
    "        x_test_gaussian1.append(t[i])\n",
    "x_test_gaussian0 = np.asarray(x_test_gaussian0, dtype='O')\n",
    "x_test_gaussian1 = np.asarray(x_test_gaussian1, dtype='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"AFTER DATASET SPLIT\"\n",
    "\n",
    "labels = [\"Is Default\", \"Not Default\"]\n",
    "total_size = y_test.size\n",
    "isDefault_count = 0\n",
    "for item in y_test:\n",
    "    if item == 1:\n",
    "        isDefault_count += 1\n",
    "print(isDefault_count, total_size)\n",
    "sizes = [isDefault_count, total_size-isDefault_count]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"STAT UTILITY FUNCTIONS\"\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers) / float(len(numbers))\n",
    "\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "\treturn sqrt(variance)\n",
    "\n",
    "def gaussian_dist_likelihood(data, mean, stdev):\n",
    "    exponent = exp(-((data - mean)**2 / (2 * stdev**2 )))\n",
    "    likelihood = (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "    return likelihood\n",
    "\n",
    "'''\n",
    "CALCULATION UTILITY FUNCTIONS\n",
    "    P(C|x) = P(x|C)*P(C) / P(x)\n",
    "    We only care about maximizing the MAP: P(x|C)*P(C)\n",
    "'''\n",
    "\n",
    "def calculate_pC(arr, target_elem): # P(C)\n",
    "    num_elem_in_class = 0\n",
    "    for a in arr:\n",
    "        if target_elem == a:\n",
    "            num_elem_in_class += 1\n",
    "\n",
    "    \"Laplacian correction\"\n",
    "    return (num_elem_in_class+1)/len(arr)\n"
   ]
  },
  {
   "source": [
    "#### Feature encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y\n",
    "y_labels = [0,1] # two possible values of isDefault\n",
    "y_store = dict()\n",
    "for label in y_labels:\n",
    "    y_store[label] = calculate_pC(y_train, int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Categorical data analysis\n",
    "Long running time alert\n",
    "'''\n",
    "\n",
    "x_store_cat = dict()\n",
    "x_labels_cat = list() # for features with categorical data only\n",
    "for feature in x_train:\n",
    "    if type(feature[0]) == str:\n",
    "        label_per_feature = list()\n",
    "        for data in feature:\n",
    "            if data not in label_per_feature:\n",
    "                label_per_feature.append(data)\n",
    "        x_labels_cat.append(label_per_feature)\n",
    "num_labels = list()\n",
    "for feature in x_labels_cat:\n",
    "    num_labels.append(len(feature))\n",
    "\n",
    "\n",
    "\"Str type column header: [grade, subGrade, employmentTitle, employmentLength, issueDate, earliestCreditLine]\"\n",
    "print(\"Number of labels for each feature column: \", num_labels)\n",
    "\n",
    "\"Since there are way too many categories in some categorical columns, it's not meaningful to label them by their categories\"\n",
    "# grade\n",
    "feature1 = x_train_str_type_only[0]\n",
    "feature1_store = dict()\n",
    "for i in range(len(feature1)-1):\n",
    "    label = feature1[i]\n",
    "    feature1_store[label] = calculate_pC(feature1, str(label))\n",
    "x_store_cat = feature1_store\n",
    "x_labels_cat = x_labels_cat[0]\n"
   ]
  },
  {
   "source": [
    "#### Model Training & prediction\n",
    "###### *Note: Implemented with the log value of the probabilities to avoid underflow*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gaussian(dataset):\n",
    "    prob_table_gaussian = list()\n",
    "    for feature in dataset.T:\n",
    "        likelihoods = list()\n",
    "        feature_mean = mean(feature)\n",
    "        feature_stdev = stdev(feature)\n",
    "        for data in feature:\n",
    "            likelihood = gaussian_dist_likelihood(data, feature_mean, feature_stdev) # P(x|C)\n",
    "            likelihoods.append(likelihood)\n",
    "        prob_table_gaussian.append(likelihoods)\n",
    "    return np.asarray(prob_table_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_class_probabilities(model, test):\n",
    "\ttotal_rows = sum([model[label][0][2] for label in model])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in model.items():\n",
    "\t\tprobabilities[class_value] = model[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, _ = class_summaries[i]\n",
    "\t\t\tprobabilities[class_value] *= calculate_probability(test[i], mean, stdev)\n",
    "\treturn probabilities\n",
    "\n",
    "def predict(x_train_gaussian, x_test_gaussian):\n",
    "\tprobabilities = calculate_class_probabilities(x_train_gaussian, x_test_gaussian)\n",
    "\tbest_label, best_prob = None, -1\n",
    "\tfor class_value, probability in probabilities.items():\n",
    "\t\tif best_label is None or probability > best_prob:\n",
    "\t\t\tbest_prob = probability\n",
    "\t\t\tbest_label = class_value\n",
    "\treturn best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(test0, test1):\n",
    "    result = list()\n",
    "    prob_table0 = fit_gaussian(test0).transpose()\n",
    "    prob_table1 = fit_gaussian(test1).transpose()\n",
    "    # print(prob_table0.shape)\n",
    "    # print(prob_table1.shape)\n",
    "    t0 = test0 # feature * data -> data * feature\n",
    "    t1 = test1\n",
    "    # print(\"t0=\",t0.shape,\"t1=\",t1.shape)\n",
    "    prob0 = 0.0\n",
    "    prob1 = 0.0\n",
    "    prob_table0 = prob_table0.transpose()\n",
    "    prob_table1 = prob_table1.transpose()\n",
    "    # For each data\n",
    "    for j in range(min(t0.shape[0],t1.shape[0])):\n",
    "        # For each feature\n",
    "        idx = min(t0.shape[1], t1.shape[1])\n",
    "        for i in range(idx):\n",
    "            \"Take the logarithm of the probabilities to avoid underflow\"\n",
    "            # print(prob_table0[j][i])\n",
    "            prob0 += log(prob_table0[i][j]) # P(x1|c)*P(x2|c)*...*P(xn|c)\n",
    "            # prob *= prob_table[j][i] # P(x1|c)*P(x2|c)*...*P(xn|c)\n",
    "        prob0 += log(x_store_cat[x_train_str_type_only[0][j]]) # categorical data\n",
    "        # prob *= (x_store_cat[x_train_str_type_only[0][j]]) # categorical data\n",
    "        prob0 += log(y_store[0])  # P(x|C) * P(C)\n",
    "\n",
    "        for i in range(idx):\n",
    "            prob1 += log(prob_table1[i][j])\n",
    "        prob1 += log(x_store_cat[x_train_str_type_only[0][j]])\n",
    "        prob1 += log(y_store[1])\n",
    "\n",
    "        if prob0 > prob1:\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return np.asarray(result)\n",
    "\n",
    "def scoring(y_test, result):\n",
    "    acc = 0\n",
    "    size = min(len(y_test),len(result))\n",
    "    for i in range(size):\n",
    "        if y_test[i] == result[i]:\n",
    "            acc += 1\n",
    "    # print(\"y_test=\",y_test)\n",
    "    # print(\"result=\",result)\n",
    "    # print(\"acc=\",acc)\n",
    "    return acc / size\n",
    "\n",
    "result = predict(x_test_gaussian0, x_test_gaussian1)\n",
    "# print(\"y_test=\",y_test[:20])\n",
    "# print(\"result=\",result)\n",
    "accuracy = scoring(y_test[:20], result)\n",
    "print(\"The test accuracy is\", accuracy)\n",
    "auc_score = roc_auc_score(np.asarray(y_test.tolist()[:len(result)]).astype(int), np.asarray(result).astype(int))\n",
    "print(\"The AUC score is\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}